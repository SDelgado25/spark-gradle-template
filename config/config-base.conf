application {

	#Application Parameters
	app-name = "spark-hdfs-processor-"${application.source-data}
	metadata-hive= ${application.layer-data-lake}"."${application.source-data}".json"


	#Spark Tunning Configuration
	spark {
		microbatch {
			interval = 1300
			time-unit= "Milliseconds"
		}
		conf {
			"spark.ui.port": "4040",
			"spark.cores.max": ${application.tunning-parameters.cores-max},
			"spark.executor.memory": ${application.tunning-parameters.executor-memory},
			"spark.executor.cores": ${application.tunning-parameters.executor-cores},
			"spark.rdd.compress": true,
			"spark.serializer": "org.apache.spark.serializer.KryoSerializer",
			"spark.io.compression.codec": "org.apache.spark.io.SnappyCompressionCodec",
			"spark.network.timeout": "600s",
			"spark.executor.heartbeatInterval":"60s",
			"spark.rpc.askTimeout":"60s",
			"spark.ui.retainedJobs": 10,
			"spark.ui.retainedStages": 100,
			"spark.ui.retainedTasks": 100,
			"spark.worker.ui.retainedExecutors": 10,
			"spark.worker.ui.retainedDrivers": 1,
			"spark.sql.ui.retainedExecutions": 10,
			"hive.exec.dynamic.partition":"true",
			"hive.exec.dynamic.partition.mode":"nonstrict",
			"spark.sql.catalogImplementation": "hive",
			"spark.sql.hive.metastore.sharedPrefixes": "com.mysql.jdbc",
			"spark.hadoop.javax.jdo.option.ConnectionDriverName": "com.mysql.jdbc.Driver",
			"spark.hadoop.javax.jdo.option.ConnectionUserName": "root",
			"spark.hadoop.javax.jdo.option.ConnectionPassword": "Ze15adv$",
			"spark.hadoop.javax.jdo.option.ConnectionURL": "jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true"
		}

	}
}